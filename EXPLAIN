Нейронная сеть использует градиентный метод оптимизации, а именно метод стохастического градиентного спуска (Stochastic Gradient Descent, SGD). Суть метода:
Модель имеет набор параметров (веса и смещения), и мы хотим найти такие их значения, при которых функция потерь (ошибка) минимальна.
1. Основной математический метод — Стохастический градиентный спуск (SGD)
На каждой итерации обучения:

Вычисляется градиент функции потерь по параметрам (частные производные).

Параметры обновляются в направлении, противоположном градиенту — туда, где функция потерь уменьшается.
2. Метод вычисления ошибки — Функция потерь CrossEntropyLoss
3. Метод активации — ReLU (Rectified Linear Unit)
ReLU делает модель нелинейной, позволяя ей приближать сложные функции.
4. Метод разделения данных — стратифицированное случайное разбиение — это метод статистической выборки, который сохраняет пропорции классов при разделении данных.
5. Внутренние методы PyTorch
PyTorch сам автоматически вычисляет градиенты через автоматическое дифференцирование (autograd).



=========================================================================================================================================================================================================================
Общая структура решения

Сбор и объединение данных.
Загружаются три исходных датасета, производится унификация названий признаков и их типов.
Все таблицы приводятся к общему набору колонок и объединяются в единый набор combined.csv.

Предобработка данных.
Выполняется удаление пустых столбцов, заполнение пропусков медианными значениями и стандартизация признаков при помощи StandardScaler.

Разделение данных.
Исходный набор делится на три части: обучающую (train), валидационную (val) и тестовую (test) в стратифицированном режиме, чтобы сохранить пропорции классов.

Преобразование данных в тензоры.
Массивы NumPy преобразуются в тензоры PyTorch, формируются датасеты (TensorDataset) и загрузчики данных (DataLoader).

Построение модели.
Определяется архитектура многослойного перцептрона с двумя скрытыми слоями и функцией активации ReLU.

Выбор функции потерь и оптимизатора.
В качестве функции потерь используется CrossEntropyLoss, а оптимизация параметров выполняется методом стохастического градиентного спуска (SGD).

Процесс обучения и валидации.
Реализуется цикл обучения с подсчётом метрик точности (accuracy) на обучающей и валидационной выборках.

Оценка качества модели.
После завершения обучения рассчитывается итоговая точность на тестовой выборке.









